{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Cognitive Search - Code Sample for chunking documents and generating vector embeddings via indexer composition\n",
    "\n",
    "This code demonstrates a pattern of composing multiple indexers to ingest content from blob storage documents, chunk them, generate embeddings and store them as their own documents in a search index. The code sample here will demonstrate storing the chunked document fragments in their own index, but users can choose to co-locate them in the same index if needed. The following image describes this composition pattern.\\\n",
    "<mark>This repo is forked from https://github.com/Azure/cognitive-search-vector-pr . The original skillset has been modified to include OCR & Merge skills in addition to existing custom web api skill. The OCR & merge skills extracts text from embedded images and image files and merges them into content before submitting it to custom skill for chunking & embedding. Changes are in section 'Utilities to manage the source document index'. An environment file credentials.env has been added to the repository that loads your credentials </mark>\n",
    "\n",
    "![Indexer chunk composition](../data/images/indexer-composition.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "To run the code, install the following packages. Please use the latest pre-release version `pip install azure-search-documents --pre`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install azure-search-documents --pre\n",
    "! pip install openai\n",
    "! pip install openai[datalib]\n",
    "! pip install python-dotenv\n",
    "! pip install azure-storage-blob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the custom web API skill for chunking + embedding\n",
    "\n",
    "This sample code also relies on a custom web api skill to be deployed to Azure functions. The custom web api skill performs chunking of content and then generates vector embeddings from the content utilizing Azure Open AI service. The code for the custom web api skill is available as an [Azure Cognitive Search power skill](https://github.com/Azure-Samples/azure-search-power-skills/blob/main/Vector/EmbeddingGenerator/README.md) and can be easily deployed via Visual Studio Code to Azure functions.\n",
    "\n",
    "Please follow those steps first and ensure that the function app is running before proceeding with the rest of the sample."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring storage accounts for deletion detection\n",
    "\n",
    "The storage accounts used for storing both the source documents as well as for storing the knowledge store projections (after the \"chunking + embedding\" step) need to adhere to the following requirements, in order to seamlessly track document deletes:\n",
    "\n",
    "1. They need to be of type \"Standard general-purpose v2\".\n",
    "2. They need to have soft delete enabled. Learn more [here](https://learn.microsoft.com/azure/storage/blobs/soft-delete-blob-enable?tabs=azure-portal)\n",
    "\n",
    "Learn more about deletion detection policies used in Azure Cognitive Search [here](https://learn.microsoft.com/azure/search/search-howto-index-changed-deleted-blobs?tabs=portal#native-blob-soft-delete-preview)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage the index, data source, skillset and indexer for the source document\n",
    "\n",
    "The following code will configure an index to hold the source documents, via an indexer that reads data from an Azure storage container that is able to generate embeddings and write that to a separate storage account (knowledge store)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import openai\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "from azure.search.documents.models import Vector  \n",
    "from azure.search.documents import SearchClient  \n",
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"credentials.env\")\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SimpleField,\n",
    "    SearchField,\n",
    "    SearchableField,\n",
    "    SearchFieldDataType,\n",
    "    SearchIndexer,\n",
    "    IndexingParameters,\n",
    "    FieldMapping,\n",
    "    FieldMappingFunction,\n",
    "    InputFieldMappingEntry, \n",
    "    OutputFieldMappingEntry, \n",
    "    SearchIndexerSkillset,\n",
    "    SearchIndexerKnowledgeStore,\n",
    "    SearchIndexerKnowledgeStoreProjection,\n",
    "    SearchIndexerKnowledgeStoreFileProjectionSelector,\n",
    "    IndexingParameters, \n",
    "    WebApiSkill,\n",
    "    OcrSkill,\n",
    "    MergeSkill,\n",
    "    SearchIndex,\n",
    "    SemanticSettings,\n",
    "    SemanticConfiguration,\n",
    "    PrioritizedFields,\n",
    "    SemanticField,\n",
    "    VectorSearch,  \n",
    "    HnswVectorSearchAlgorithmConfiguration,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_SEARCH_SERVICE_ENDPOINT = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "AZURE_SEARCH_KEY = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "AZURE_SEARCH_KNOWLEDGE_STORE_CONNECTION_STRING = os.getenv(\"AZURE_KNOWLEDGE_STORE_STORAGE_CONNECTION_STRING\")\n",
    "\n",
    "def get_index_client() -> SearchIndexClient:\n",
    "    return SearchIndexClient(AZURE_SEARCH_SERVICE_ENDPOINT, AzureKeyCredential(AZURE_SEARCH_KEY))\n",
    "\n",
    "def get_indexer_client() -> SearchIndexerClient:\n",
    "    return SearchIndexerClient(AZURE_SEARCH_SERVICE_ENDPOINT, AzureKeyCredential(AZURE_SEARCH_KEY))\n",
    "\n",
    "def get_index_name(index_prefix):\n",
    "    return f\"{index_prefix}-index\"\n",
    "\n",
    "def get_datasource_name(index_prefix):\n",
    "    return f\"{index_prefix}-datasource\"\n",
    "\n",
    "def get_skillset_name(index_prefix):\n",
    "    return f\"{index_prefix}-skillset\"\n",
    "\n",
    "def get_indexer_name(index_prefix):\n",
    "    return f\"{index_prefix}-indexer\"\n",
    "\n",
    "def get_chunk_index_blob_container_name(index_prefix):\n",
    "    return f\"{index_prefix}ChunkIndex\".replace('-', '').lower()\n",
    "\n",
    "def get_knowledge_store_connection_string():\n",
    "    return AZURE_SEARCH_KNOWLEDGE_STORE_CONNECTION_STRING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define simple utilities to to help configure index, data source, skillset (with knowledge store) and indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(index_name, fields, vector_search, semantic_title_field_name, semantic_content_field_names):\n",
    "    semantic_settings = SemanticSettings(\n",
    "        configurations=[SemanticConfiguration(\n",
    "            name='default',\n",
    "            prioritized_fields=PrioritizedFields(\n",
    "                title_field=SemanticField(field_name=semantic_title_field_name), prioritized_content_fields=[SemanticField(field_name=field_name) for field_name in semantic_content_field_names]))])\n",
    "    index = SearchIndex(\n",
    "        name=index_name,\n",
    "        fields=fields,\n",
    "        vector_search=vector_search,\n",
    "        semantic_settings=semantic_settings)\n",
    "    index_client = get_index_client()\n",
    "    return index_client.create_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blob_datasource(datasource_name, storage_connection_string, container_name):\n",
    "    # This example utilizes a REST request as the python SDK doesn't support the blob soft delete policy yet\n",
    "    api_version = '2023-07-01-Preview'\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'api-key': f'{AZURE_SEARCH_KEY}'\n",
    "    }\n",
    "    data_source = {\n",
    "        \"name\": datasource_name,\n",
    "        \"type\": \"azureblob\",\n",
    "        \"credentials\": {\"connectionString\": storage_connection_string},\n",
    "        \"container\": {\"name\": container_name},\n",
    "        \"dataDeletionDetectionPolicy\": {\"@odata.type\": \"#Microsoft.Azure.Search.NativeBlobSoftDeleteDeletionDetectionPolicy\"}\n",
    "    }\n",
    "\n",
    "    url = '{}/datasources/{}?api-version={}'.format(AZURE_SEARCH_SERVICE_ENDPOINT, datasource_name, api_version)\n",
    "    response = requests.put(url, json=data_source, headers=headers)\n",
    "\n",
    "    ds_client = get_indexer_client()\n",
    "    return ds_client.get_data_source_connection(datasource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_indexer_completion(indexer_name):\n",
    "    indexer_client = get_indexer_client()\n",
    "    # poll status and wait until indexer is complete\n",
    "    status = f\"Indexer {indexer_name} not started yet\"\n",
    "    while (indexer_client.get_indexer_status(indexer_name).last_result == None) or ((status := indexer_client.get_indexer_status(indexer_name).last_result.status) != \"success\"):\n",
    "        print(f\"Indexing status:{status}\")\n",
    "\n",
    "        # It's possible that the indexer may reach a state of transient failure, especially when generating embeddings\n",
    "        # via Open AI. For the purposes of the demo, we'll just break out of the loop and continue with the rest of the steps.\n",
    "        if (status == \"transientFailure\"):\n",
    "            print(f\"Indexer {indexer_name} failed before fully indexing documents\")\n",
    "            break\n",
    "        time.sleep(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities to manage the \"source\" document index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentIndexManager():\n",
    "    def _create_document_index(self, index_prefix):\n",
    "        name = get_index_name(index_prefix)\n",
    "        fields = [\n",
    "            SimpleField(name=\"document_id\", type=SearchFieldDataType.String, filterable=True, sortable=True, key=True),\n",
    "            SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "            SimpleField(name=\"filesize\", type=SearchFieldDataType.Int64),\n",
    "            SimpleField(name=\"filepath\", type=SearchFieldDataType.String)\n",
    "        ]\n",
    "        return create_index(name, fields, vector_search=None, semantic_title_field_name=\"filepath\", semantic_content_field_names=[\"content\"])\n",
    "\n",
    "    def _create_document_datasource(self, index_prefix, storage_connection_string, container_name):\n",
    "        name = get_datasource_name(index_prefix)\n",
    "        return create_blob_datasource(name, storage_connection_string, container_name)\n",
    "\n",
    "    def _create_document_skillset(self, index_prefix, content_field_name=\"content\"):\n",
    "        embedding_skill_endpoint = os.getenv(\"AZURE_SEARCH_EMBEDDING_SKILL_ENDPOINT\")\n",
    "\n",
    "        name = get_skillset_name(index_prefix)\n",
    "        chunk_index_blob_container_name = get_chunk_index_blob_container_name(index_prefix)\n",
    "        content_context = f\"/document/{content_field_name}\"\n",
    "        embedding_skill = WebApiSkill(\n",
    "                            name=\"chunking-embedding-skill\",\n",
    "                            uri=embedding_skill_endpoint,\n",
    "                            timeout=\"PT3M\",\n",
    "                            batch_size=1,\n",
    "                            degree_of_parallelism=1,\n",
    "                            context=content_context,\n",
    "                            inputs=[\n",
    "                                    InputFieldMappingEntry(name=\"document_id\", source=\"/document/document_id\"),\n",
    "                                    InputFieldMappingEntry(name=\"text\", source=\"/document/merged_text\"),\n",
    "                                    InputFieldMappingEntry(name=\"filepath\", source=\"/document/filepath\"),\n",
    "                                    InputFieldMappingEntry(name=\"fieldname\", source=f\"='{content_field_name}'\")],\n",
    "                            outputs=[OutputFieldMappingEntry(name=\"chunks\", target_name=\"chunks\")])\n",
    "        ocr_skill = OcrSkill(\n",
    "                            name=\"ocr-skill\",\n",
    "                            description=\"Extracts text (plain and structured) from image.\",\n",
    "                            context=\"/document/normalized_images/*\",\n",
    "                            default_language_code=\"en\",\n",
    "                            should_detect_orientation=True,\n",
    "                            inputs=[\n",
    "                                    InputFieldMappingEntry(name=\"image\", source=\"/document/normalized_images/*\")],\n",
    "                            outputs=[\n",
    "                                    OutputFieldMappingEntry(name=\"text\", target_name=\"text\")])\n",
    "\n",
    "        merge_skill = MergeSkill(\n",
    "                            name=\"merge-skill\",\n",
    "                            context=\"/document\",\n",
    "                            insert_pre_tag=\" \",\n",
    "                            insert_post_tag=\" \",\n",
    "                            inputs=[\n",
    "                                    InputFieldMappingEntry(name=\"text\", source=\"/document/content\"),\n",
    "                                    InputFieldMappingEntry(name=\"itemsToInsert\", source=\"/document/normalized_images/*/text\"),\n",
    "                                    InputFieldMappingEntry(name=\"offsets\", source=\"/document/normalized_images/*/contentOffset\")],\n",
    "                            outputs=[\n",
    "                                    OutputFieldMappingEntry(name=\"mergedText\", target_name=\"merged_text\")])\n",
    "\n",
    "        knowledge_store = SearchIndexerKnowledgeStore(storage_connection_string=get_knowledge_store_connection_string(),\n",
    "                                                    projections=[\n",
    "                                                                SearchIndexerKnowledgeStoreProjection(\n",
    "                                                                    objects=[SearchIndexerKnowledgeStoreFileProjectionSelector(\n",
    "                                                                        storage_container=chunk_index_blob_container_name,\n",
    "                                                                        generated_key_name=\"id\",\n",
    "                                                                        source_context=f\"{content_context}/chunks/*\",\n",
    "                                                                        inputs=[\n",
    "                                                                            InputFieldMappingEntry(name=\"source_document_id\", source=\"/document/document_id\"),\n",
    "                                                                            InputFieldMappingEntry(name=\"source_document_filepath\", source=\"/document/filepath\"),\n",
    "                                                                            InputFieldMappingEntry(name=\"source_document_url\", source=\"/document/url\"),\n",
    "                                                                            InputFieldMappingEntry(name=\"source_field_name\", source=f\"{content_context}/chunks/*/embedding_metadata/fieldname\"),\n",
    "                                                                            InputFieldMappingEntry(name=\"title\", source=f\"{content_context}/chunks/*/title\"),\n",
    "                                                                            InputFieldMappingEntry(name=\"text\", source=f\"{content_context}/chunks/*/content\"),\n",
    "                                                                            InputFieldMappingEntry(name=\"embedding\", source=f\"{content_context}/chunks/*/embedding_metadata/embedding\"),\n",
    "                                                                            InputFieldMappingEntry(name=\"index\", source=f\"{content_context}/chunks/*/embedding_metadata/index\"),\n",
    "                                                                            InputFieldMappingEntry(name=\"offset\", source=f\"{content_context}/chunks/*/embedding_metadata/offset\"),\n",
    "                                                                            InputFieldMappingEntry(name=\"length\", source=f\"{content_context}/chunks/*/embedding_metadata/length\")                                                                            \n",
    "                                                                            ]\n",
    "                                                                            )\n",
    "                                                                    ]),\n",
    "                                                                SearchIndexerKnowledgeStoreProjection(\n",
    "                                                                files=[SearchIndexerKnowledgeStoreFileProjectionSelector(\n",
    "                                                                    storage_container=f\"{chunk_index_blob_container_name}images\",\n",
    "                                                                    generated_key_name=\"imagepath\",\n",
    "                                                                    source=\"/document/normalized_images/*\",\n",
    "                                                                    inputs=[]\n",
    "                                                                        )\n",
    "                                                                ])\n",
    "                                                                ])\n",
    "        skillset = SearchIndexerSkillset(name=name, skills=[ocr_skill,merge_skill,embedding_skill], description=name, knowledge_store=knowledge_store)\n",
    "        client = get_indexer_client()\n",
    "        return client.create_skillset(skillset)\n",
    "\n",
    "    def _create_document_indexer(self, index_prefix, data_source_name, index_name, skillset_name, content_field_name=\"content\", generate_page_images=True):\n",
    "        content_context = f\"/document/{content_field_name}\"\n",
    "        name = get_indexer_name(index_prefix)\n",
    "        indexer_config = {\"dataToExtract\": \"contentAndMetadata\", \"imageAction\": \"generateNormalizedImages\"} if generate_page_images else {\"dataToExtract\": \"contentAndMetadata\"}\n",
    "        parameters = IndexingParameters(max_failed_items = -1, configuration=indexer_config)\n",
    "        indexer = SearchIndexer(\n",
    "            name=name,\n",
    "            data_source_name=data_source_name,\n",
    "            target_index_name=index_name,\n",
    "            skillset_name=skillset_name,\n",
    "            field_mappings=[FieldMapping(source_field_name=\"metadata_storage_path\", target_field_name=\"document_id\", mapping_function=FieldMappingFunction(name=\"base64Encode\", parameters=None)),\n",
    "                            FieldMapping(source_field_name=\"metadata_storage_name\", target_field_name=\"filepath\"),\n",
    "                            FieldMapping(source_field_name=\"metadata_storage_path\", target_field_name=\"url\"),\n",
    "                            FieldMapping(source_field_name=\"metadata_storage_size\", target_field_name=\"filesize\")],\n",
    "            output_field_mappings=[],\n",
    "            parameters=parameters\n",
    "        )\n",
    "        indexer_client = get_indexer_client()\n",
    "        return indexer_client.create_indexer(indexer)\n",
    "\n",
    "    def create_document_index_resources(self, index_prefix, customer_storage_connection_string, customer_container_name) -> dict:\n",
    "        index_name = self._create_document_index(index_prefix).name\n",
    "        data_source_name = self._create_document_datasource(index_prefix, customer_storage_connection_string, customer_container_name).name\n",
    "        skillset_name = self._create_document_skillset(index_prefix).name    \n",
    "        time.sleep(5)\n",
    "        indexer_name = self._create_document_indexer(index_prefix, data_source_name, index_name, skillset_name).name\n",
    "        wait_for_indexer_completion(indexer_name)\n",
    "        return {\"index_name\": index_name, \"data_source_name\": data_source_name, \"skillset_name\": skillset_name, \"indexer_name\": indexer_name}\n",
    "\n",
    "    def delete_document_index_resources(self, index_prefix):\n",
    "        index_client = get_index_client()\n",
    "        indexer_client = get_indexer_client()\n",
    "\n",
    "        index_client.delete_index(index=get_index_name(index_prefix))\n",
    "        indexer_client.delete_indexer(indexer=get_indexer_name(index_prefix))\n",
    "        indexer_client.delete_data_source_connection(data_source_connection=get_datasource_name(index_prefix))\n",
    "        indexer_client.delete_skillset(skillset=get_skillset_name(index_prefix))\n",
    "\n",
    "        # delete the knowledge store tables and blobs\n",
    "        knowledge_store_connection_string  = get_knowledge_store_connection_string()\n",
    "        \n",
    "        # delete the container directly from storage\n",
    "        try:\n",
    "            blob_service = BlobServiceClient.from_connection_string(knowledge_store_connection_string)\n",
    "            blob_service.delete_container(get_chunk_index_blob_container_name(index_prefix))\n",
    "        # handle resource not found error\n",
    "        except ResourceNotFoundError:\n",
    "            pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities to manage the \"chunked\" document index - with vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkIndexManager():\n",
    "\n",
    "    def _create_chunk_index(self, index_prefix):\n",
    "        name = get_index_name(f\"{index_prefix}-chunk\")\n",
    "        vector_search = VectorSearch(\n",
    "            algorithm_configurations=[\n",
    "                HnswVectorSearchAlgorithmConfiguration(\n",
    "                    name=\"my-vector-config\",\n",
    "                    kind=\"hnsw\",\n",
    "                    parameters={\n",
    "                        \"m\": 4,\n",
    "                        \"efConstruction\": 400,\n",
    "                        \"efSearch\": 1000,\n",
    "                        \"metric\": \"cosine\"\n",
    "                    }\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        fields = [\n",
    "            SimpleField(name=\"id\", type=SearchFieldDataType.String, facetable=True, filterable=True, sortable=True, key=True),            \n",
    "            SimpleField(name=\"source_document_id\", type=SearchFieldDataType.String),\n",
    "            SimpleField(name=\"source_document_filepath\", type=SearchFieldDataType.String),\n",
    "            SimpleField(name=\"source_document_url\", type=SearchFieldDataType.String),\n",
    "            SimpleField(name=\"source_field_name\", type=SearchFieldDataType.String),\n",
    "            SearchableField(name=\"title\", type=SearchFieldDataType.String),   \n",
    "            SimpleField(name=\"index\", type=SearchFieldDataType.Int64),\n",
    "            SimpleField(name=\"offset\", type=SearchFieldDataType.Int64),\n",
    "            SimpleField(name=\"length\", type=SearchFieldDataType.Int64),\n",
    "            SimpleField(name=\"hash\", type=SearchFieldDataType.String),\n",
    "            SearchableField(name=\"text\", type=SearchFieldDataType.String),                 \n",
    "            SearchField(name=\"embedding\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, vector_search_dimensions=1536, vector_search_configuration=\"my-vector-config\")    \n",
    "        ]\n",
    "        index = create_index(name, fields, vector_search=vector_search, semantic_title_field_name=\"title\", semantic_content_field_names=[\"text\"])\n",
    "        return index\n",
    "    \n",
    "    def _create_chunk_datasource(self, index_prefix, storage_connection_string, container_name):\n",
    "        name = get_datasource_name(f\"{index_prefix}-chunk\")\n",
    "        return create_blob_datasource(name, storage_connection_string, container_name)\n",
    "\n",
    "    def _create_chunk_indexer(self, index_prefix, data_source_name, index_name):\n",
    "        name = get_indexer_name(f\"{index_prefix}-chunk\")\n",
    "        parameters = IndexingParameters(configuration={\"parsing_mode\": \"json\"})\n",
    "        indexer = SearchIndexer(\n",
    "            name=name,\n",
    "            data_source_name=data_source_name,\n",
    "            target_index_name=index_name,\n",
    "            parameters=parameters\n",
    "        )\n",
    "        indexer_client = get_indexer_client()\n",
    "        return indexer_client.create_indexer(indexer)\n",
    "\n",
    "\n",
    "    def create_chunk_index_resources(self, index_prefix) -> dict:\n",
    "        chunk_index_storage_connection_string = get_knowledge_store_connection_string()\n",
    "        chunk_index_blob_container_name = get_chunk_index_blob_container_name(index_prefix)\n",
    "\n",
    "        index_name = self._create_chunk_index(index_prefix).name\n",
    "        data_source_name = self._create_chunk_datasource(index_prefix, chunk_index_storage_connection_string, chunk_index_blob_container_name).name\n",
    "        time.sleep(5)\n",
    "        indexer_name = self._create_chunk_indexer(index_prefix, data_source_name, index_name).name\n",
    "        wait_for_indexer_completion(indexer_name)\n",
    "        return {\"index_name\": index_name, \"data_source_name\": data_source_name, \"indexer_name\": indexer_name}\n",
    "\n",
    "\n",
    "    # delete all the resources\n",
    "    def delete_chunk_index_resources(self, index_prefix):\n",
    "        index_client = get_index_client()\n",
    "        indexer_client = get_indexer_client()\n",
    "\n",
    "        index_client.delete_index(index=f\"{index_prefix}-chunk-index\")\n",
    "        indexer_client.delete_indexer(indexer=f\"{index_prefix}-chunk-indexer\")\n",
    "        indexer_client.delete_data_source_connection(data_source_connection=f\"{index_prefix}-chunk-datasource\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text embedder utility to aid during query time\n",
    "\n",
    "**NOTE**: Make sure to utilize the same Azure OpenAI Embedding Deployment at query time as the one used in the custom web api skill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbedder():\n",
    "    openai.api_type = \"azure\"    \n",
    "    openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    openai.api_base = f\"https://{os.getenv('AZURE_OPENAI_SERVICE_NAME')}.openai.azure.com/\"\n",
    "    openai.api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "    AZURE_OPENAI_EMBEDDING_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\")\n",
    "\n",
    "    def clean_text(self, text, text_limit=7000):\n",
    "        # Clean up text (e.g. line breaks, )    \n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        text = re.sub(r'[\\n\\r]+', ' ', text).strip()\n",
    "        # Truncate text if necessary (e.g. for, ada-002, 4095 tokens ~ 7000 chracters)    \n",
    "        if len(text) > text_limit:\n",
    "            logging.warning(\"Token limit reached exceeded maximum length, truncating...\")\n",
    "            text = text[:text_limit]\n",
    "        return text\n",
    "\n",
    "    # Function to generate embeddings for title and content fields, also used for query embeddings\n",
    "    def generate_embeddings(self, text, clean_text=True):\n",
    "        if clean_text:\n",
    "            text = self.clean_text(text)\n",
    "        response = openai.Embedding.create(input=text, engine=self.AZURE_OPENAI_EMBEDDING_DEPLOYMENT)\n",
    "        embeddings = response['data'][0]['embedding']\n",
    "        return embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wire up the utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indexes(prefix, customer_storage_connection_string, container_name):\n",
    "    index_manager = DocumentIndexManager()\n",
    "    doc_index_resources = index_manager.create_document_index_resources(prefix, customer_storage_connection_string, container_name)\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    chunk_index_manager = ChunkIndexManager()\n",
    "    chunk_index_resources = chunk_index_manager.create_chunk_index_resources(prefix)\n",
    "    return {\"doc_index_resources\": doc_index_resources, \"chunk_index_resources\": chunk_index_resources}\n",
    "\n",
    "def delete_indexes(prefix):\n",
    "    index_manager = DocumentIndexManager()\n",
    "    index_manager.delete_document_index_resources(prefix)\n",
    "    chunk_index_manager = ChunkIndexManager()\n",
    "    chunk_index_manager.delete_chunk_index_resources(prefix)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "The following code will upload a bunch of sample PDFs to the \"source document\" storage account, in the container specified. And will implement the indexer composition pattern to ingest both the content from the source documents as well as the chunked + embedded content."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the sample data to blob storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenant ='pythonsample'\n",
    "\n",
    "customer_storage_connection_string = os.getenv(\"DOCUMENT_AZURE_STORAGE_CONNECTION_STRING\")\n",
    "container_name = os.getenv(\"DOCUMENT_AZURE_STORAGE_CONTAINER_NAME\")\n",
    "\n",
    "prefix = f\"{tenant}-{container_name}\"\n",
    "\n",
    "# Delete any existing Azure Cognitive Search resources\n",
    "delete_indexes(prefix)\n",
    "\n",
    "blob_service_client = BlobServiceClient.from_connection_string(customer_storage_connection_string)\n",
    "container_client = blob_service_client.get_container_client(container=container_name)\n",
    "\n",
    "if not container_client.exists():\n",
    "    container_client.create_container()\n",
    "\n",
    "# Upload sample documents to blob storage\n",
    "for root, dirs, files in os.walk(\"../data/documents/\"):\n",
    "    for file in files:\n",
    "        with open(os.path.join(root, file), \"rb\") as data:\n",
    "            container_client.upload_blob(file, data, overwrite=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Azure Cognitive Search resources\n",
    "\n",
    "**NOTE**: The following example creates the source document indexer and the chunk document indexer, but we wait for the first indexer to fully finish its run before creating the second - this is reasonable with very small amounts of data, but wouldn't scale well for larger data. In that scenario it would make more sense to create the indexers in parallel with a schedule and let them run on their own and converge eventually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure indexes\n",
    "index_resources = create_indexes(prefix, customer_storage_connection_string, container_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the \"chunk\" search index with different kinds of queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_vector_index(index_name, query, vector_only=False):  \n",
    "    embedder = TextEmbedder()  \n",
    "    vector = Vector(value=embedder.generate_embeddings(query), k=3, fields=\"embedding\")  \n",
    "    search_client = SearchClient(AZURE_SEARCH_SERVICE_ENDPOINT, index_name, AzureKeyCredential(AZURE_SEARCH_KEY))  \n",
    "    if vector_only:  \n",
    "        search_text = None  \n",
    "    else:  \n",
    "        search_text = query  \n",
    "    results = search_client.search(search_text=search_text, vectors=[vector], top=3)  \n",
    "    return results  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vector only query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_index_name = index_resources[\"chunk_index_resources\"][\"index_name\"]  \n",
    "results = query_vector_index(chunk_index_name, \"hearing aid\", vector_only=True)  \n",
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Content: {result['text']}\")  \n",
    "    print(f\"Source Document: {result['source_document_filepath']}\")  \n",
    "    print(f\"Citation: {result['source_document_url']}\")  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hybrid query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_index_name = index_resources[\"chunk_index_resources\"][\"index_name\"]\n",
    "results = query_vector_index(chunk_index_name, \"hearing aid\")\n",
    "for result in results:\n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Content: {result['text']}\")  \n",
    "    print(f\"Source Document: {result['source_document_filepath']}\")  \n",
    "    print(f\"Citation: {result['source_document_url']}\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
